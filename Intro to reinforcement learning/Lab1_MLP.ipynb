{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Overview\n",
    "\n",
    "## Introduction\n",
    "- Assume a neural network composed of three layers of neurons: input layer, hidden layer and output layer. We wish to train this network so when it is presented with a specific input it will have a certain output.\n",
    "- For this lab we are going to work with the [MNIST database of handwritten digits](http://yann.lecun.com/exdb/mnist) and train our network to recognise handwritten 0-9 digits. The MNIST database contains 60000 images which can used to train the network and 10000 more images for testing purposes. Each image is an  matrix representing a digit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Lab1_NetworkScheme.jpg\" width=\"800\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The neural network\n",
    "\n",
    "### Network initialisation\n",
    "\n",
    "\n",
    "1. Generate a set of weights between the input and the hidden layer. The input layer should have 784 neurons, one for each pixel of the image.\n",
    "\n",
    "2. Generate a set of weights between the hidden and the output layer. The output layer should have 10 neurons, one for each digit.\n",
    "\n",
    "3. Generate two sets of bias, one for the hidden layer and one for the output layer. Bias is set to 0 to start with \n",
    "\n",
    "\n",
    "### Feedforward\n",
    "\n",
    "\n",
    "1. Feed an image $\\vec{x(\\mu)}$ to the network,\n",
    "   \n",
    "\n",
    "2. Compute the input to each of the neurons of the hidden layer, $h^{(1)}_i = \\sum_{i=1}^{784}w^{(1)}_{ij} x^{(0)}_j + b^{(1)}_i$ and their outputs with the use of the [sigmoid function](http://mathworld.wolfram.com/SigmoidFunction.html),$x^{(1)}_i = f(h^{(1)}_i) = \\frac{1}{(1+e^{-h^{(1)}_i})}$.\n",
    "\n",
    "3. Repeat for the output layer, $h^{(2)}_i = \\sum_{i=1}^{784}w^{(2)}_{ij} x^{(1)}_j + b^{(2)}_i$ and their outputs, $x^{(2)}_i = f(h^{(2)}_i)$\n",
    "\n",
    "<img src=\"nn2.jpg\"  width=\"800\" height=\"600\">\n",
    "\n",
    "\n",
    "### Error\n",
    "\n",
    "1. Calculate the error in the output of the neural network. If the input is an image of a zero digit it is expected that the first neuron of the input layer to be fully active, i.e has an output of 1, while the other neurons have outputs of 0. Thus the target output would be $t = [1,0,0,\\dots,0]^T$ and, given that the output of the neural network is some $output$, the error is given by the formula $$E(\\mu) = \\sum_{i=1}^{n}\\frac{1}{2}(t_i(\\mu)-x^{(2)}_i(\\mu))^2,$$ where $n$ is the number of neurons in the output layer of the network and $\\mu$ is sample index. The total error per sample is then given by $$E/N = \\sum_{\\mu=1}^N \\sum_{i=1}^{n}\\frac{1}{2N}(t_i(\\mu)-x^{(2)}_i(\\mu))^2.$$\n",
    "\n",
    "\n",
    "\n",
    "### Backpropagation\n",
    "Here we will consider explicitly the logistic sigmoid activation function so we can use the result that the derivative of the logistic sigmoid is\n",
    "$$ f'(h) = \\frac{\\partial f(h) }{\\partial h } = f(h)(1-f(h)) $$ directly in the update rules.\n",
    "\n",
    "1. Apply the delta rule between the output and the hidden layers. For the weight $w_{ij}$ we have \n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{\\textrm{ij}} }=\\frac{\\partial E}{\\partial x^{(2)}_i }\\;\\frac{\\partial x^{(2)}_i }{\\partial h^{(2)}_i } \\; \\frac{\\partial h^{(2)}_i }{\\partial w_{\\textrm{ij}} } \\; =-\\left( t_i -x^{(2)}_i \\right)\\; x^{(2)}_i \\;\\left(1-x^{(2)}_i \\right) \\; x^{(1)}_j,$$ \n",
    "\n",
    "and we can define  $\\delta^{(2)}_i = \\left(t_i - x^{(2)}_i \\right)x^{(2)}_i \\;\\left(1-x^{(2)}_i \\right) $.\n",
    "\n",
    "2. Update the weights between the hidden and the output layers.  For the weight $w_{ij}$ we have the update after a single input $${\\Delta w}_{\\textrm{ij}} = \\eta \\delta^{(2)}_i x^{(1)}_j.$$\n",
    "\n",
    "3. Repeat steps 1 and 2 between the hidden and the input layes and update the weights between them. Here for the weight $w_{ij}$ we have \n",
    "$$\\frac{\\partial E}{\\partial w_{\\textrm{ij}} }=\\frac{\\partial E}{\\partial x^{(1)}_j }\\;\\frac{\\partial x^{(1)}_j }{\\partial h^{(1)}_j }\\;\\frac{\\partial h^{(1)}_j }{\\partial w_{\\textrm{ij}} }\n",
    "=x^{(0)}_j x^{(1)}_i \\;\\left(1-x^{(1)}_i \\right) \\sum_k \\left(w^{(2)}_{\\textrm{ki}} \\delta^{(2)}_k \\right),$$ where the terms $w_{\\textrm{ij}}^{\\left(2\\right)}, \\delta_j^{\\left(2\\right)}$ are the weights and the delta function of the previous layer. So now we can defined the backpropagated local gradient for the hidden layer as\n",
    "$$ \\delta_i^{(1)} = x^{(1)}_i \\;\\left(1-x^{(1)}_i \\right) \\sum_j \\delta^{(2)}_j w^{(2)}_{ji}. $$\n",
    "\n",
    "4. Update the weights between the hidden and the input layers. For the weight $w_{ij}$ we have $${\\Delta w}_{\\textrm{ij}} =\\eta \\delta^{(1)}_i x^{(0)}_j.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "The code provided first loads the MNIST database and generates a neural network using the parameters provided. Then it trains the network and tests it using the testing set of the MNIST database.\n",
    "\n",
    "1. Test the network using the test dataset, then compute and comment on it's accuracy.\n",
    "\n",
    "\n",
    "2. The weights are generated from a uniform distribution in the interval (0,1) and are then normalized using the sum equals to 1 normalization, i.e the summation of the inputs should be equal to 1. Why is this normalization important? Another initialisation method is the [Xavier initialization](http://www.jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf?hc_location=ufi), which is more commonly used in deep neural networks of more than one hidden layer. In this initialisation the weights are initialised using normally distributed random numbers which are then scaled by sqrt(1/n_cols). Change your initialisation to this and comment on how this changes the training.\n",
    "\n",
    "\n",
    "3. The weights are updated through mini-batches. Change the mini-batch size and see the effect that this has on performance and\n",
    "   computational speed. What are the limits in defining a mini-batch size?\n",
    "\n",
    "\n",
    "4. A bias is included in the calculation of the activation but is set to zero and is not trained. Derive the learning rule and code the training of the bias alongside the weights. It may help to think of the bias as an extra weight where the input value is fixed to 1.\n",
    "\n",
    "\n",
    "5. Upgrade the network by adding one more hidden layer, derive the appropriate formulas for the feedforward and the backpropagation and program them.\n",
    "\n",
    "\n",
    "6. Try to use ten classes and to optimise the network hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset:\n",
    "#     The MNIST dataset contains 60,000 training samples.\n",
    "#     To start with we will limit this to 5 classes (the first 5 digits in the dataset) to simplify development\n",
    "#     Reading the data file gives an array of img_size by no. of samples\n",
    "#     so we will work on the transposed array instead \n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "mnist = loadmat('MNIST.mat')\n",
    "\n",
    "# Read the train set\n",
    "x_train = mnist['x_train']\n",
    "# Read the train labels\n",
    "trainlabels = mnist['trainlabels']\n",
    "\n",
    "# Read the test set\n",
    "x_test = mnist['x_test']\n",
    "# Read the test labels\n",
    "testlabels = mnist['testlabels']\n",
    "\n",
    "#We select the data for three classes that we want to classify\n",
    "\n",
    "N_class=5\n",
    "\n",
    "Index_tr=[]\n",
    "Index_te=[]\n",
    "\n",
    "for i in range(N_class):\n",
    "    \n",
    "    #Find the indexes of the training set corresponding to class i\n",
    "    ind_tr=np.where(trainlabels==i)[0]\n",
    "    #Find the indexes of the test set corresponding to class i\n",
    "    ind_te=np.where(testlabels==i)[0]\n",
    "    \n",
    "    # Append the training indexes in a list\n",
    "    Index_tr.append(ind_tr)\n",
    "    # Append the testing indexes in a list\n",
    "    Index_te.append(ind_te)\n",
    "\n",
    "# Reshape the lists to be a 1-d array    \n",
    "Index_tr=np.concatenate(Index_tr,axis=0)\n",
    "Index_te=np.concatenate(Index_te,axis=0)\n",
    "\n",
    "# Create a training set and a test set with data belonging to the classes considered only\n",
    "x_train=np.copy(x_train[Index_tr,:])\n",
    "x_test=np.copy(x_test[Index_te,:])\n",
    "\n",
    "# Compute the size of the train and test datasets\n",
    "N_tr=np.shape(Index_tr)[0]\n",
    "N_te=np.shape(Index_te)[0]\n",
    "\n",
    "# Create one-hot encoding labels for train and test datasets\n",
    "y_train=np.zeros([N_tr,N_class])\n",
    "y_test=np.zeros([N_te,N_class])\n",
    "\n",
    "for i in range(N_tr):\n",
    "    \n",
    "    y_train[i,int(trainlabels[Index_tr[i]])]=1\n",
    "\n",
    "for i in range(N_te):\n",
    "\n",
    "    y_test[i,int(testlabels[Index_te[i]])]=1\n",
    "\n",
    "    \n",
    "n_samples, img_size = x_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAOv0lEQVR4nO3df6zV9X3H8deLuysqioFaKKV2VIVa5laot1hnW2xNDbpkaFLbksUy50KTVofVbTVuSU2XLK6xde2K7WilYn9gmqiVNM5KGZmztdQLUkHRYikowmCCm7/xXu57f9yvy1Xv93MO53zPD+7n+Uhuzrnf9/mc7zsHXvd7zvmc7/k4IgRg7BvX6QYAtAdhBzJB2IFMEHYgE4QdyMTvtXNnR3l8HK0J7dwlkJVX9KJejYMerdZU2G0vkPQ1ST2SvhMR16duf7Qm6Eyf28wuASSsj7WltYafxtvukbRM0vmSZktaZHt2o/cHoLWaec0+T9ITEbE9Il6VdJukhdW0BaBqzYR9uqSnRvy+q9j2OraX2O633T+gg03sDkAzmgn7aG8CvOmztxGxPCL6IqKvV+Ob2B2AZjQT9l2SThrx+zsk7W6uHQCt0kzYH5Q00/a7bB8l6VOSVlfTFoCqNTz1FhGDti+X9FMNT72tiIhHKusMQKWammePiLsl3V1RLwBaiI/LApkg7EAmCDuQCcIOZIKwA5kg7EAmCDuQCcIOZIKwA5kg7EAmCDuQCcIOZIKwA5kg7EAmCDuQCcIOZIKwA5kg7EAmCDuQCcIOZIKwA5lo65LNGHsGP3pGsr7ns+VLfv36rJXJse99YHGy/vZlRyXrPes2Juu54cgOZIKwA5kg7EAmCDuQCcIOZIKwA5kg7EAmmGdH0tD8ucn611d8I1k/tbf8v9hQjX0/dNZ3k/XH+w4l638z4wM19pCXpsJue4ek5yUdkjQYEX1VNAWgelUc2T8SEc9UcD8AWojX7EAmmg17SLrX9gbbS0a7ge0ltvtt9w+o/HPSAFqr2afxZ0fEbttTJK2x/VhE3DfyBhGxXNJySZroydHk/gA0qKkje0TsLi73SbpT0rwqmgJQvYbDbnuC7eNfuy7pPElbqmoMQLWaeRo/VdKdtl+7nx9GxD2VdIW2GTgvPVv6tzd9L1mf1Zs+p3woMZu+fWAgOfZ/h8Yn63PTZR08//2ltWPWbU6OHXrllfSdH4EaDntEbJf03gp7AdBCTL0BmSDsQCYIO5AJwg5kgrADmeAU1zGgZ+LE0tqLHz4tOfbzN/4wWf/IMS/U2Hvjx4tbnv3jZH3tTWcl6z+/7uvJ+prvfKu0Nvv7lyfHnvyFB5L1IxFHdiAThB3IBGEHMkHYgUwQdiAThB3IBGEHMsE8+xiw69bppbUH37+sjZ0cni9NeTBZv+e49Dz8pTvOS9ZXzvhZaW3i7P3JsWMRR3YgE4QdyARhBzJB2IFMEHYgE4QdyARhBzLBPPsRYPCjZyTrq+aUL5s8Tumveq7l0p3nJuv9P3tPsr75svLe1r18dHLslP6Xk/Unnk2fq9/7j+tKa+OcHDomcWQHMkHYgUwQdiAThB3IBGEHMkHYgUwQdiATjoi27WyiJ8eZTs/b5mho/txk/Z9X3pSsn9rb+Mcl/vSxi5L1no+/mKwf+JN3J+v7Ty+f0J617Knk2MGndiXrtfzk6Q2ltT2H0nP4f7H4r5L1nnUbG+qp1dbHWj0XB0Z90Gse2W2vsL3P9pYR2ybbXmN7W3E5qcqGAVSvnqfxt0ha8IZt10haGxEzJa0tfgfQxWqGPSLuk3TgDZsXSlpZXF8p6cJq2wJQtUbfoJsaEXskqbicUnZD20ts99vuH9DBBncHoFktfzc+IpZHRF9E9PVqfKt3B6BEo2Hfa3uaJBWX+6prCUArNBr21ZIWF9cXS7qrmnYAtErNCVrbqySdI+lE27skfVHS9ZJ+ZPsySU9KuriVTR7pfMYfJOvPXJWe853Vmz4nfUPirZB/f2F2cuz+205K1t/ybHqd8hO+/8t0PVEbTI5srak96ZeU+698KVmfUn6qfNeqGfaIWFRS4tMxwBGEj8sCmSDsQCYIO5AJwg5kgrADmeCrpCsw7thjk/XBLz+XrP/ytDuS9d8NvpqsX3Xt1aW1Sf/5ZHLslAnpz0MdSlbHrnnTdibrO9rTRqU4sgOZIOxAJgg7kAnCDmSCsAOZIOxAJgg7kAnm2Svw8vz0Kaw/PS39VdC1/OXSzyfrx/+4/DTTTp5Giu7CkR3IBGEHMkHYgUwQdiAThB3IBGEHMkHYgUwwz16BP/qHTcn6uBp/Uy/dmf6i3mN+/KvDbQmSet1TWhuosVJ5j9u3lHm7cGQHMkHYgUwQdiAThB3IBGEHMkHYgUwQdiATzLPX6X8uOau09vdTb0iOHVKNJZfvTS+r/E79IlnH6Aai/FvvhzSUHHvP1vS/yUxtbKinTqp5ZLe9wvY+21tGbLvO9tO2NxU/F7S2TQDNqudp/C2SFoyy/caImFP83F1tWwCqVjPsEXGfpANt6AVACzXzBt3lth8unuZPKruR7SW2+233D+hgE7sD0IxGw/5NSadImiNpj6SvlN0wIpZHRF9E9PVqfIO7A9CshsIeEXsj4lBEDEn6tqR51bYFoGoNhd32tBG/XiRpS9ltAXSHmvPstldJOkfSibZ3SfqipHNsz5EUGl6q+jOta7E7DB5TXjthXHoe/YFX0i9fTr51d3rfyerYVWvd+8duOL3GPWworfzZ9vOTI09b+rtk/Uhct75m2CNi0Sibb25BLwBaiI/LApkg7EAmCDuQCcIOZIKwA5ngFNc22H/ouGR9cPuO9jTSZWpNrT1+/R8m648t/Eay/m8vnVBa273s1OTY458tXwb7SMWRHcgEYQcyQdiBTBB2IBOEHcgEYQcyQdiBTDDP3gZ//fOLk/VZiVMxj3RD8+eW1vZd9XJy7Na+9Dz6uZs/maxPWLC9tHa8xt48ei0c2YFMEHYgE4QdyARhBzJB2IFMEHYgE4QdyATz7PVyeWlcjb+ZX/vgqmR9mWY10lFX2Pml8qWsJen2T3+1tDarN/0V3O/71eJk/e0XPZqs4/U4sgOZIOxAJgg7kAnCDmSCsAOZIOxAJgg7kAnm2esV5aUhDSWHzj9mf7J+5S1nJOunfDd9/73/9Xxpbe/8tybHTv7krmT9ineuTdbPPzZ9Lv7qF6eW1j69eUFy7In/OiFZx+GpeWS3fZLtdba32n7E9tJi+2Tba2xvKy4ntb5dAI2q52n8oKSrI+I9kj4g6XO2Z0u6RtLaiJgpaW3xO4AuVTPsEbEnIjYW15+XtFXSdEkLJa0sbrZS0oUt6hFABQ7rDTrbMyTNlbRe0tSI2CMN/0GQNKVkzBLb/bb7B3SwyXYBNKrusNs+TtLtkq6MiOfqHRcRyyOiLyL6ejW+kR4BVKCusNvu1XDQfxARdxSb99qeVtSnSdrXmhYBVKHm1JttS7pZ0taIGHm+4mpJiyVdX1ze1ZIOx4CjnX6Yt37sW8n6/R86OlnfdvBtpbVLT9iRHNuspbs/lKzf84s5pbWZS/P7OudOqmee/WxJl0jabHtTse1aDYf8R7Yvk/SkpPSXowPoqJphj4j7Vf7VDedW2w6AVuHjskAmCDuQCcIOZIKwA5kg7EAmHJE4d7NiEz05zvSR+QZ+z6xTSmuzVu1Mjv2ntz3Q1L5rfVV1rVNsUx46mL7vRf+xJFmfdenYXW76SLQ+1uq5ODDq7BlHdiAThB3IBGEHMkHYgUwQdiAThB3IBGEHMsFXSdfp0G9+W1rbdvGM5NjZV1yRrD/6iX9ppKW6nHb3Z5P1d9/0UrI+6yHm0ccKjuxAJgg7kAnCDmSCsAOZIOxAJgg7kAnCDmSC89mBMYTz2QEQdiAXhB3IBGEHMkHYgUwQdiAThB3IRM2w2z7J9jrbW20/Yntpsf0620/b3lT8XND6dgE0qp4vrxiUdHVEbLR9vKQNttcUtRsj4obWtQegKvWsz75H0p7i+vO2t0qa3urGAFTrsF6z254haa6k9cWmy20/bHuF7UklY5bY7rfdP6CDzXULoGF1h932cZJul3RlRDwn6ZuSTpE0R8NH/q+MNi4ilkdEX0T09Wp88x0DaEhdYbfdq+Gg/yAi7pCkiNgbEYciYkjStyXNa12bAJpVz7vxlnSzpK0R8dUR26eNuNlFkrZU3x6AqtTzbvzZki6RtNn2pmLbtZIW2Z4jKSTtkPSZFvQHoCL1vBt/v6TRzo+9u/p2ALQKn6ADMkHYgUwQdiAThB3IBGEHMkHYgUwQdiAThB3IBGEHMkHYgUwQdiAThB3IBGEHMkHYgUy0dclm2/8taeeITSdKeqZtDRyebu2tW/uS6K1RVfb2+xHx1tEKbQ37m3Zu90dEX8caSOjW3rq1L4neGtWu3ngaD2SCsAOZ6HTYl3d4/ynd2lu39iXRW6Pa0ltHX7MDaJ9OH9kBtAlhBzLRkbDbXmD7cdtP2L6mEz2Usb3D9uZiGer+DveywvY+21tGbJtse43tbcXlqGvsdai3rljGO7HMeEcfu04vf9721+y2eyT9RtLHJO2S9KCkRRHxaFsbKWF7h6S+iOj4BzBsf1jSC5JujYjTi21flnQgIq4v/lBOiogvdElv10l6odPLeBerFU0bucy4pAsl/bk6+Ngl+vqE2vC4deLIPk/SExGxPSJelXSbpIUd6KPrRcR9kg68YfNCSSuL6ys1/J+l7Up66woRsSciNhbXn5f02jLjHX3sEn21RSfCPl3SUyN+36XuWu89JN1re4PtJZ1uZhRTI2KPNPyfR9KUDvfzRjWX8W6nNywz3jWPXSPLnzerE2EfbSmpbpr/Ozsi3ifpfEmfK56uoj51LePdLqMsM94VGl3+vFmdCPsuSSeN+P0dknZ3oI9RRcTu4nKfpDvVfUtR731tBd3icl+H+/l/3bSM92jLjKsLHrtOLn/eibA/KGmm7XfZPkrSpySt7kAfb2J7QvHGiWxPkHSeum8p6tWSFhfXF0u6q4O9vE63LONdtsy4OvzYdXz584ho+4+kCzT8jvxvJf1dJ3oo6etkSb8ufh7pdG+SVmn4ad2Ahp8RXSbpLZLWStpWXE7uot6+J2mzpIc1HKxpHertgxp+afiwpE3FzwWdfuwSfbXlcePjskAm+AQdkAnCDmSCsAOZIOxAJgg7kAnCDmSCsAOZ+D/cBlFxmLMWWwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of image, take the transpose to see it in the right orientation\n",
    "example_image=np.reshape(x_train[0],(28,28))\n",
    "plt.imshow(example_image.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on Batch size: On decreasing batch size, training time is increasing but accuracy is increasing. I decided to keep mini-batch size to be 8 since it is not too slow and still have good accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of epochs is a hyperparameter that defines the number times that the learning algorithm \n",
    "# will work through the entire training dataset.\n",
    "\n",
    "# The batch size is a hyperparameter that defines the number of samples to work through before \n",
    "# updating the internal model parameters. \n",
    "\n",
    "# ref: https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/\n",
    "\n",
    "n_epoch = 10\n",
    "batch_size = 8\n",
    "n_batches = int(math.floor(n_samples/batch_size))\n",
    "\n",
    "# define the size of each of the layers in the network\n",
    "n_input_layer  = img_size\n",
    "n_hidden_layer = 100\n",
    "n_output_layer = N_class\n",
    "\n",
    "# Add another hidden layer\n",
    "n_hidden_layer2 = 100 # number of neurons of the hidden layer. 0 deletes this layer\n",
    "\n",
    "# eta is the learning rate\n",
    "eta = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a simple network\n",
    "# For W1 and W2 columns are the input and the rows are the output.\n",
    "# W1: Number of columns (input) needs to be equal to the number of features \n",
    "#     of the  MNIST digits, thus p. Number of rows (output) should be equal \n",
    "#     to the number of neurons of the hidden layer thus n_hidden_layer.\n",
    "# W2: Number of columns (input) needs to be equal to the number of neurons \n",
    "#     of the hidden layer. Number of rows (output) should be equal to the \n",
    "#     number of digits we wish to find (classification).\n",
    "xavier_init = True\n",
    "if xavier_init:\n",
    "    W1 = np.random.rand(n_hidden_layer, n_input_layer) * np.sqrt(1/n_input_layer)\n",
    "    \n",
    "    if n_hidden_layer2 > 0:\n",
    "        W2 = np.random.rand(n_hidden_layer2, n_hidden_layer) * np.sqrt(1/n_hidden_layer)\n",
    "        W3 = np.random.rand(n_output_layer, n_hidden_layer2) * np.sqrt(1/n_hidden_layer2)\n",
    "    else:\n",
    "        W2 = np.random.rand(n_output_layer, n_hidden_layer) * np.sqrt(1/n_hidden_layer)\n",
    "else:\n",
    "    W1 = np.random.uniform(0,1,(n_hidden_layer, n_input_layer))\n",
    "    W2 = np.random.uniform(0,1,(n_output_layer, n_hidden_layer))\n",
    "\n",
    "    # The following normalises the random weights so that the sum of each row =1\n",
    "    W1 = np.divide(W1,np.matlib.repmat(np.sum(W1,1)[:,None],1,n_input_layer))\n",
    "    W2 = np.divide(W2,np.matlib.repmat(np.sum(W2,1)[:,None],1,n_hidden_layer))\n",
    "\n",
    "    if n_hidden_layer2 > 0:\n",
    "        W3 = np.random.uniform(0,1,(n_output_layer, n_hidden_layer2))\n",
    "        W3 = np.divide(W1,np.matlib.repmat(np.sum(W1,1)[:,None],1,n_hidden_layer2))\n",
    "        W2 = np.random.uniform(0,1,(n_hidden_layer2, n_hidden_layer))\n",
    "        W2 = np.divide(W2,np.matlib.repmat(np.sum(W2,1)[:,None],1,n_hidden_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the biases\n",
    "bias_W1 = np.zeros((n_hidden_layer,))\n",
    "bias_W2 = np.zeros((n_output_layer,))\n",
    "if n_hidden_layer2 > 0:\n",
    "    bias_W3 = np.zeros((n_output_layer,))\n",
    "    bias_W2 = np.zeros((n_hidden_layer2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of the network inputs and average error per epoch\n",
    "errors = np.zeros((n_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 : error =  0.40139199504086837\n",
      "Epoch  2 : error =  0.08948289620508193\n",
      "Epoch  3 : error =  0.02686328870876849\n",
      "Epoch  4 : error =  0.022464737592535387\n",
      "Epoch  5 : error =  0.019451468114737865\n",
      "Epoch  6 : error =  0.016938270476553653\n",
      "Epoch  7 : error =  0.01511798567336655\n",
      "Epoch  8 : error =  0.013458967539702878\n",
      "Epoch  9 : error =  0.012151010883024324\n",
      "Epoch  10 : error =  0.010993989665701621\n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "\n",
    "for i in range(0,n_epoch):\n",
    "\n",
    "    # We will shuffle the order of the samples each epoch\n",
    "    shuffled_idxs = np.random.permutation(n_samples)\n",
    "    \n",
    "    for batch in range(0,n_batches):\n",
    "        \n",
    "        # Initialise the gradients for each batch\n",
    "        dW1 = np.zeros(W1.shape)\n",
    "        dW2 = np.zeros(W2.shape)\n",
    "\n",
    "        dbias_W1 = np.zeros(bias_W1.shape)\n",
    "        dbias_W2 = np.zeros(bias_W2.shape)\n",
    "\n",
    "        if n_hidden_layer2 > 0:\n",
    "            dW3 = np.zeros(W3.shape)\n",
    "            dbias_W3 = np.zeros(bias_W3.shape)\n",
    "    \n",
    "        # Loop over all the samples in the batch\n",
    "        for j in range(0,batch_size):\n",
    "\n",
    "            # Input (random element from the dataset)\n",
    "            idx = shuffled_idxs[batch*batch_size + j]\n",
    "            x0 = x_train[idx]\n",
    "            \n",
    "            # Neural activation: input layer -> hidden layer\n",
    "            h1 = np.dot(W1,x0)+bias_W1\n",
    "\n",
    "            # Apply the sigmoid function\n",
    "            x1 = 1/(1+np.exp(-h1))\n",
    "\n",
    "            # Neural activation: hidden layer -> output layer\n",
    "            h2 = np.dot(W2,x1)+bias_W2\n",
    "\n",
    "            # Apply the sigmoid function\n",
    "            x2 = 1/(1+np.exp(-h2))\n",
    "            # Form the desired output, the correct neuron should have 1 the rest 0\n",
    "            desired_output = y_train[idx]\n",
    "            \n",
    "            if n_hidden_layer2 > 0:\n",
    "                h3 = np.dot(W3, x2) + bias_W3\n",
    "                x3 = 1/(1+np.exp(-h3))\n",
    "                \n",
    "                # Compute the error signal\n",
    "                e_n = desired_output - x3\n",
    "                \n",
    "                # Backpropagation: output layer -> hidden layer2\n",
    "                delta3 = x3*(1-x3)*e_n\n",
    "                dW3 += np.outer(delta3,x2)\n",
    "                dbias_W3 += delta3\n",
    "                \n",
    "                # Backpropagation: hidden layer2 -> hidden layer\n",
    "                delta2 = x2*(1-x2) * np.dot(W3.T, delta3)\n",
    "                \n",
    "            else:\n",
    "                # Compute the error signal\n",
    "                e_n = desired_output - x2\n",
    "\n",
    "                # Backpropagation: output layer -> hidden layer\n",
    "                delta2 = x2*(1-x2) * e_n\n",
    "\n",
    "            dW2 += np.outer(delta2, x1)\n",
    "            dbias_W2 += delta2\n",
    "\n",
    "            # Backpropagation: hidden layer -> input layer\n",
    "            delta1 = x1*(1-x1) * np.dot(W2.T, delta2)\n",
    "            dW1 += np.outer(delta1,x0)\n",
    "            dbias_W1 += delta1\n",
    "\n",
    "            # Store the error per epoch\n",
    "            errors[i] = errors[i] + 0.5*np.sum(np.square(e_n))/n_samples\n",
    "\n",
    "        # After each batch update the weights using accumulated gradients\n",
    "        W2 += eta*dW2/batch_size\n",
    "        W1 += eta*dW1/batch_size\n",
    "    \n",
    "        bias_W1 += eta*dbias_W1/batch_size\n",
    "        bias_W2 += eta*dbias_W2/batch_size\n",
    "        \n",
    "        if n_hidden_layer2 > 0:\n",
    "            W3 += eta*dW3/batch_size\n",
    "            bias_W3 += eta*dbias_W3/batch_size\n",
    "        \n",
    "    print( \"Epoch \", i+1, \": error = \", errors[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the performance\n",
    "plt.plot(errors)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Average error per epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.65732632807939\n"
     ]
    }
   ],
   "source": [
    "# Test the dataset and find the accuracy\n",
    "num_test = x_test.shape[0]\n",
    "predicted_labels = np.zeros(num_test)\n",
    "actual_labels = np.zeros(num_test)\n",
    "correctly_classified = 0\n",
    "for idx in range(0, num_test):\n",
    "    x0 = x_test[idx]        \n",
    "    # Neural activation: input layer -> hidden layer\n",
    "    h1 = np.dot(W1,x0)+bias_W1\n",
    "\n",
    "    # Apply the sigmoid function\n",
    "    x1 = 1/(1+np.exp(-h1))\n",
    "\n",
    "    # Neural activation: hidden layer -> output layer\n",
    "    h2 = np.dot(W2,x1)+bias_W2\n",
    "\n",
    "    # Apply the sigmoid function\n",
    "    x2 = 1/(1+np.exp(-h2))\n",
    "    if n_hidden_layer2 > 0:\n",
    "        h3 = np.dot(W3,x2)+bias_W3\n",
    "        x3 = 1/(1+np.exp(-h3))\n",
    "        predicted_labels[idx] = np.argmax(x3)\n",
    "    else:\n",
    "        predicted_labels[idx] = np.argmax(x2)\n",
    "\n",
    "    actual_labels[idx] = np.argmax(y_test[idx])\n",
    "    if predicted_labels[idx] == actual_labels[idx]:\n",
    "        correctly_classified += 1\n",
    "\n",
    "accuracy = correctly_classified*100/num_test\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
