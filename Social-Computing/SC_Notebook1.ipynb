{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zOAtdMCqAC9"
      },
      "source": [
        "## Social Computing: Notebook 1\n",
        "# Online data collection using the requests library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tS1zepSqAC_"
      },
      "outputs": [],
      "source": [
        "# Please inlcude your names below and edit the name of the file to include the names of the people answering\n",
        "\n",
        "# Students: Rupal Saxena, Orestis Oikonomou"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9fc_jlrqADA"
      },
      "outputs": [],
      "source": [
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bInHXoQqADA"
      },
      "source": [
        "As step 0, pick your favorite Wikipedia page, open it in the browser, and then save it as an html file. Now open it in the browser as well as in a text editor and look at the difference. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAPqi9QNqADB"
      },
      "source": [
        "Using the requests library you can retrieve the html source of the page, in a response object (using requests.get(“url”)). The response object you received has content that you can access calling the .text function on it.\n",
        "\n",
        "Call text and save the result in a file, then open the file in a browser and check whether you successfully saved the page. Note, you will only be able to open the file in the browser if you give it an html extension."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## our code\n",
        "filename = \"getbillie.html\"\n",
        "res = requests.get(\"https://en.wikipedia.org/wiki/Billie_Eilish\")\n",
        "file = open(filename, \"w\")\n",
        "file.write(res.text)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "v_llSXDCtPzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjoW5fDRqADB"
      },
      "source": [
        "### 1) Basic web crawling (10 points)\n",
        "\n",
        "URLs have specific formats, for example any Wikipedia page will be of the format https://en.wikipedia.org/wiki/Pythonidae where the last word is the topic of the article.\n",
        "Next, we want to automate this saving process using the requests library and making automated requests to Wikipedia.\n",
        "\n",
        "Exercise: Pick 5 different words, and write code that loops through these words, and retrieves the html content for each associated wikipedia page, and saves the html text as wiki_htmls/[word].html files. (Choose words that actually have associated wiki pages). \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToOU3dfOqADB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists('wiki_htmls'):\n",
        "    os.makedirs('wiki_htmls')\n",
        "\n",
        "words = [\"Billboard_200\", \"Time_(magazine)\", \"Brit_Awards\", \"Golden_Globe_Award_for_Best_Original_Song\", \"MTV_Video_Music_Awards\"]\n",
        "\n",
        "for i in words:\n",
        "  filename = \"wiki_htmls/\"+i+\".html\"\n",
        "  res = requests.get(\"https://en.wikipedia.org/wiki/\"+i)\n",
        "  file = open(filename, \"w\")\n",
        "  file.write(res.text)\n",
        "  file.close()\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqYa1q1TqADC"
      },
      "source": [
        "### 2) URL formats (10 points)\n",
        "\n",
        "What is the common URL in the case of Google searches? And in the case of Yelp? "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In case of Google searches the common URL is:**\n",
        "https://www.google.com/search?q=\n",
        "\n",
        "**In case of Yelp the common URL is:**\n",
        "https://de.yelp.ch/search?\n"
      ],
      "metadata": {
        "id": "hxXCE3InrOk8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJdQIg7XqADD"
      },
      "source": [
        "And what happens to the URL if you want to define the location as well as the type of venue you are looking for?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   If we want to define the location the url changes and its core part becomes:https://de.yelp.ch/search?find_desc= \n",
        "*   If we want to define the venue changes and its core part becomes:https://de.yelp.ch/search?find_desc=&find_loc="
      ],
      "metadata": {
        "id": "1p-aCSpRyM3N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZimqtAt_qADE"
      },
      "source": [
        "Can you find more search parameters for either of the two pages that you can define via the URLs? What do they mean?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   When we change the filter for price range the url becomes: https://de.yelp.ch/search?find_desc=&find_loc=zurich&attrs= . After, *attrs* follows a variable that defines the price range.\n",
        "*   When we change the filter for distance the url becomes: https://de.yelp.ch/search?find_desc=&find_loc=zurich&l= . After, *l* follows a different serie of characters depending on the covered region that we chose.\n",
        "\n"
      ],
      "metadata": {
        "id": "jMYqsXyyzCp-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8JU5NTTqADE"
      },
      "source": [
        "### 3) HTML content basics (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgsPxynRqADF"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "res = requests.get(\"http://dataquestio.github.io/web-scraping-pages/ids_and_classes.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7aL3WsiqADF"
      },
      "source": [
        "Using the BeautifulSoup parser library we will parse the webpage that you just saved. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3OSQx7nqADF"
      },
      "outputs": [],
      "source": [
        "# let's import BeautifulSoup, our parser library\n",
        "# And make a soup object out of the html of the page\n",
        "\n",
        "# in case bs4 throws error try\n",
        "# !pip install --upgrade html5lib==1.0b8\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(res.content, 'html.parser')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y99LV_wEqADF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4958ad3-6128-4ab6-a0bc-ef5df947f4b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<html>\n",
            " <head>\n",
            "  <title>\n",
            "   A simple example page\n",
            "  </title>\n",
            " </head>\n",
            " <body>\n",
            "  <div>\n",
            "   <p class=\"inner-text first-item\" id=\"first\">\n",
            "    First paragraph.\n",
            "   </p>\n",
            "   <p class=\"inner-text\">\n",
            "    Second paragraph.\n",
            "   </p>\n",
            "  </div>\n",
            "  <p class=\"outer-text first-item\" id=\"second\">\n",
            "   <b>\n",
            "    First outer paragraph.\n",
            "   </b>\n",
            "  </p>\n",
            "  <p class=\"outer-text\">\n",
            "   <b>\n",
            "    Second outer paragraph.\n",
            "   </b>\n",
            "  </p>\n",
            " </body>\n",
            "</html>\n"
          ]
        }
      ],
      "source": [
        "# print a nice version using prettify\n",
        "print(soup.prettify())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38r5Ra2PqADF"
      },
      "source": [
        "Here's how we can find all instances of a tag at once: Try to predict what the following command will return: `soup.find_all('p')` and then call it to check if you were right. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Prediction***\n",
        "\n",
        "It will print all the parts of the html find that are defined as paragraphs."
      ],
      "metadata": {
        "id": "ojwMygsH0VPX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0S0i7zEqADG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "720d11e2-4705-47fc-9dbc-f0e08414b017"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<p class=\"inner-text first-item\" id=\"first\">\n",
              "                 First paragraph.\n",
              "             </p>, <p class=\"inner-text\">\n",
              "                 Second paragraph.\n",
              "             </p>, <p class=\"outer-text first-item\" id=\"second\">\n",
              " <b>\n",
              "                 First outer paragraph.\n",
              "             </b>\n",
              " </p>, <p class=\"outer-text\">\n",
              " <b>\n",
              "                 Second outer paragraph.\n",
              "             </b>\n",
              " </p>]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "soup.find_all('p')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLK2lmWyqADG"
      },
      "source": [
        "Print out the second element of this list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPT_W74QqADG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "302625a4-508b-4684-a6bb-597afbbf7ffb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<p class=\"inner-text\">\n",
            "                Second paragraph.\n",
            "            </p>\n"
          ]
        }
      ],
      "source": [
        "print(soup.find_all(\"p\")[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkPWOtqlqADG"
      },
      "source": [
        "Print out the text inside the second element of the list, using the .text on the element."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-oVL2X4qADG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99fc60fe-163a-4312-fcb4-5adebd606685"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "                Second paragraph.\n",
            "            \n"
          ]
        }
      ],
      "source": [
        "print(soup.find_all(\"p\")[1].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp4ykrg6qADG"
      },
      "source": [
        "When you try to find a specific element on a page you can reach it by finding classes or IDs of the elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nttv14z1qADG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e1d97ed-260c-4d6e-cb3f-50f478f165b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<p class=\"outer-text first-item\" id=\"second\">\n",
              " <b>\n",
              "                 First outer paragraph.\n",
              "             </b>\n",
              " </p>, <p class=\"outer-text\">\n",
              " <b>\n",
              "                 Second outer paragraph.\n",
              "             </b>\n",
              " </p>]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "soup.find_all('p', class_='outer-text')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "aHG_ux5AqADG"
      },
      "source": [
        "How many elements would it return for 'inner-text'? Guess, and check your guess by using the find_all command"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Our guess***\n",
        "\n",
        "It will return 2"
      ],
      "metadata": {
        "id": "PDJrcNUZ1jgi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ot2SUP4XqADH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57ab8db9-3146-45ba-c30b-a1d769fa1271"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<p class=\"inner-text first-item\" id=\"first\">\n",
              "                 First paragraph.\n",
              "             </p>, <p class=\"inner-text\">\n",
              "                 Second paragraph.\n",
              "             </p>]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "soup.find_all('p', class_='inner-text')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "7J54nhY7qADH"
      },
      "source": [
        "### 4) Finding elements in the browser (50 points)\n",
        "Since every web page is different and html can get very large and messy, the easiest way to find elements that you are interested in is to start from the browser window. So next we will quickly look at how to find elements using the developer tools in your browser. Open the following webpage in your browser (preferably Chrome): http://forecast.weather.gov/MapClick.php?lat=21.3049&lon=-157.8579#.Wkwh8VQ-fVo \n",
        "\n",
        "Find the developer tools in your browser. (In Chrome, it's view --> developer --> developer tools or Control+Shift+C on Windows and Command+Shift+C on Mac) You should end up with a panel at the bottom or the right side of the browser like what you see below. Make sure the Elements panel is highlighted:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OB6d9NcBqADH"
      },
      "outputs": [],
      "source": [
        "res = requests.get(\"http://forecast.weather.gov/MapClick.php?lat=21.3049&lon=-157.8579\")\n",
        "soup = BeautifulSoup(res.content, 'html.parser')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8FKjVQcqADH"
      },
      "source": [
        "When trying to find a specific element, you can right click on it on the page and select \"inspect\". This will also open up the developer tools window. For example if we want to extract the current temperature value:\n",
        "\n",
        "<img src=\"inspect.png\">\n",
        "\n",
        "<img src=\"inspect_class.png\">\n",
        "\n",
        "<br><br>\n",
        "1. (5 points) Using the find function, extract and print out the current temperature from the page.\n",
        "2. (5 points) Do the same with the value in Celsius."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZJ_qA8yqADH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ba5a63f-91e4-44ba-9da8-a27e844fb608"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The current temprature in Fahrenheit is 80°F\n",
            "The current temprature in Celsius is 27°C\n"
          ]
        }
      ],
      "source": [
        "### Fill out and print a full sentence describing the temperature in F and C. \n",
        "temp_F = soup.find('p', class_='myforecast-current-lrg').text\n",
        "print(\"The current temprature in Fahrenheit is \"+ temp_F)\n",
        "temp_C = soup.find('p', class_='myforecast-current-sm').text\n",
        "print(\"The current temprature in Celsius is \"+temp_C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ic8VfaBqADH"
      },
      "source": [
        "3. (20 points) In this exercise we will extract each half day's forecast from the extended forecast on the weather report page. <br>\n",
        "    a. Find the container for the extended forecast on the weather page we just downloaded. <br>\n",
        "    b. Make a list with all forecast items (overnight, Wednesday, Wednesday night, etc) <br>\n",
        "    c. For each time period, print out the name of the period, the short description of the expected weather conditions, and the temperature. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I think it can be just this\n",
        "forecast= soup.find_all('li', class_=\"forecast-tombstone\")\n",
        "for i in forecast:\n",
        "  print(i.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_jXAUhjaJMv",
        "outputId": "93f7a69c-cb6d-4f6f-c63c-c414968e72d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Today\n",
            "ScatteredShowersHigh: 84 °F\n",
            "\n",
            "\n",
            "Tonight\n",
            "ScatteredShowersLow: 73 °F\n",
            "\n",
            "\n",
            "Thursday\n",
            "Mostly Sunnyand BreezyHigh: 83 °F\n",
            "\n",
            "\n",
            "ThursdayNight\n",
            "ScatteredShowers andBreezyLow: 72 °F\n",
            "\n",
            "\n",
            "Friday\n",
            "ScatteredShowers andBreezyHigh: 83 °F\n",
            "\n",
            "\n",
            "FridayNight\n",
            "IsolatedShowers andBreezyLow: 72 °F\n",
            "\n",
            "\n",
            "Saturday\n",
            "IsolatedShowers andBreezyHigh: 83 °F\n",
            "\n",
            "\n",
            "SaturdayNight\n",
            "Mostly Clearthen IsolatedShowersLow: 72 °F\n",
            "\n",
            "\n",
            "Sunday\n",
            "IsolatedShowersHigh: 83 °F\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L4NOgbWqADH"
      },
      "source": [
        "4. (20 points) Take a list of jobs (e.g.['teacher', 'lawyer', 'data-scientist']). For each job save the html of the result of searching it on indeed. The url of a result page looks like: https://www.indeed.com/q-data-scientist-jobs.html. \n",
        "<br>\n",
        "For each job find the names of the companies from the first result page.  Make a dictionary where the keys are the jobs and value is a list of the company names. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnG7RP5NqADI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5609145-8506-4e4b-ba93-bb7c142ce102"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'data-scientist': ['Nebraska Furniture Mart', 'Union Pacific', 'Spotify', 'BlueSky Technology Solutions', 'OTC Direct Inc', 'Accentuate Staffing', 'WorkCog', 'Amazon Web Services, Inc.', 'CDC Foundation ( Contract)', 'TheIncLab', 'Zoom Video Communications, Inc.', 'Lark Health', 'Millennium Health', 'Bayer', 'Radcube LLC'], 'teacher': ['Council Bluffs Community School District', 'Millard Public Schools', 'Council Bluffs Community School District', 'Millard Public Schools', 'US Bureau of Indian Education', 'Tinkergarten', 'US Bureau of Indian Education', 'Council Bluffs Community School District', 'ST. THOMAS MORE CATHOLIC SCHOOL', 'Teach Iowa', 'Council Bluffs Community School District', 'North Kingstown School Department', 'Teach Iowa', 'Glenwood CSD', 'Bellevue Public Schools'], 'lawyer': ['PSI Services LLC', 'PACCAR', 'US National Labor Relations Board', 'Contract Counselors', 'Animal Defense Legal Fund', 'U.S. Department of Education Office of General...', 'Spotify', 'US DHS Headquarters', 'Gilbert Harrell Sumerford & Martin, P.C.', 'U.S. Northern Command', 'Matts Staffing (Excalibur Security)', 'Stanislaus County', 'Union Pacific', 'RLG', 'City of Anaheim, CA']}\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "companies = {}\n",
        "\n",
        "response_ds = requests.get(\"https://www.indeed.com/q-data-scientist-jobs.html\")\n",
        "soup_ds = BeautifulSoup(response_ds.content, 'html.parser')\n",
        "html_list = soup_ds.find_all('span', class_=\"companyName\")\n",
        "\n",
        "companies[\"data-scientist\"] = []\n",
        "\n",
        "for company in html_list:\n",
        "  name = company.text\n",
        "  companies[\"data-scientist\"].append(name)\n",
        "\n",
        "response_ds = requests.get(\"https://www.indeed.com/jobs?q=teacher&l&vjk=df5f6bb62e33874f\")\n",
        "soup_ds = BeautifulSoup(response_ds.content, 'html.parser')\n",
        "html_list = soup_ds.find_all('span', class_=\"companyName\")\n",
        "\n",
        "companies[\"teacher\"] = []\n",
        "\n",
        "for company in html_list:\n",
        "  name = company.text\n",
        "  companies[\"teacher\"].append(name)\n",
        "\n",
        "\n",
        "response_ds = requests.get(\"https://www.indeed.com/jobs?q=lawyer&l&vjk=dd4d90c192144f6b\")\n",
        "soup_ds = BeautifulSoup(response_ds.content, 'html.parser')\n",
        "html_list = soup_ds.find_all('span', class_=\"companyName\")\n",
        "\n",
        "companies[\"lawyer\"] = []\n",
        "\n",
        "for company in html_list:\n",
        "  name = company.text\n",
        "  companies[\"lawyer\"].append(name)\n",
        "\n",
        "print(companies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7htagNKqADI"
      },
      "source": [
        "### 5) Headers (25 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FZ9-8C3qADI"
      },
      "source": [
        "Every request you send has a so called HTTP header (unrelated to the content of the message), for example to communicate the size of the message, the browser from which the request is coming from, or what kind of response it is expecting back in the response. \n",
        "\n",
        "1) Read up on this: What parts does a request contain exactly and what is the purpose of a header? \n",
        "\n",
        "2) Look in the browser: Take a URL and find the request header using the developer tools in your browser. (Hint: you will need to look inside 'network'). \n",
        "\n",
        "3) If you don’t tell python otherwise, it will use a default header when sending requests. What is this default when you use the requests library?\n",
        "\n",
        "4) The requests library allows to specify the headers of your request exactly. Set the header of your request (for the  URL you previously picked) to be the one copied from your browser. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F61jUXPUqADI"
      },
      "source": [
        "5) Now compare the response headers for the same URL in the browser, and by calling a function on the response object in your code. What differences do you see? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv5MhBk9qADI"
      },
      "source": [
        "**Your chosen URL:** \n",
        "https://en.wikipedia.org/wiki/Billie_Eilish\n",
        "\n",
        "**Default header of Python requests:** ##\n",
        "{'User-Agent': 'python-requests/2.23.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}\n",
        "\n",
        "**Header in your browser:** ##\n",
        "\n",
        ":authority: en.wikipedia.org\n",
        ":method: GET\n",
        ":path: /wiki/Billie_Eilish\n",
        ":scheme: https\n",
        "accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\n",
        "accept-encoding: gzip, deflate, br\n",
        "accept-language: en-GB,en-US;q=0.9,en;q=0.8 ..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.get(\"https://en.wikipedia.org/wiki/Billie_Eilish\")\n",
        "print(response.request.headers)"
      ],
      "metadata": {
        "id": "bt87kLvTpYrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng0SSuv8qADI"
      },
      "source": [
        "**Response header in your browser:** ##\n",
        "\n",
        "accept-ranges: bytes\n",
        "age: 70951\n",
        "cache-control: private, s-maxage=0, max-age=0, must-revalidate\n",
        "content-encoding: gzip\n",
        "content-language: en\n",
        "content-length: 122758\n",
        "content-type: text/html; charset=UTF-8\n",
        "date: Tue, 15 Mar 2022 16:26:12 GMT\n",
        "last-modified: Tue, 15 Mar 2022 16:23:12 GMT\n",
        "\n",
        "**Response header in the response in python: **##\n",
        "\n",
        "{'User-Agent': 'python-requests/2.23.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}\n",
        "\n",
        "**Difference: **##"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "SC_Notebook1_Rupal_Orestis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}